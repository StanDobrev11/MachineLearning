{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1362a891-720b-48bd-a322-a49cc52be301",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import prepare_dataframe, generate_training_dataframe, split_dataframe, manage_prediction, create_real_pred_df, plot_tds, probability_within_radius\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.stats import norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9de97d0d-1c25-408c-add1-7390573ade48",
   "metadata": {},
   "source": [
    "# North Pacific Ocean Tropical Depressions Novel Forecast Model\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Building upon the foundational [analysis of the effects of the El Niño-Southern Oscillation (ENSO) on tropical depressions (TDs) in the North Pacific Ocean](https://github.com/StanDobrev11/enso_effect_on_npacific_tds), this current project takes the research a step further by developing a machine learning (ML) forecast model. The aim is to predict the track, intensity, and progression of tropical depressions based on their initial characteristics and ENSO phases.\n",
    "\n",
    "In the previous project, correlations between ENSO phases and various tropical depression characteristics, such as frequency, intensity, and storm tracks were successfully identified, especially in the NE Pacific region. It was established that El Niño and La Niña events have discernible impacts on where and how frequently TDs form, as well as their overall intensity across the North Pacific region. This prior work provided valuable insights into how different ENSO conditions influence tropical depressions, laying a strong foundation for more advanced predictive modeling.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Brief explanation of ENSO and TDs\n",
    "\n",
    "The **El Niño-Southern Oscillation (ENSO)** is a climate phenomenon characterized by fluctuations in sea surface temperatures and atmospheric conditions in the Pacific Ocean. ENSO consists of three phases: **El Niño**, when ocean waters in the central and eastern Pacific are warmer than average; **La Niña**, when these waters are cooler than normal; and a **Neutral** phase when conditions are closer to the long-term average. ENSO has significant global impacts, affecting weather patterns, rainfall, and storm activity.\n",
    "\n",
    "A **Tropical Depression (TD)** is a low-pressure weather system that typically develops around 5 degrees north or south of the equator. These systems are the early stage of tropical cyclones and can intensify into stronger storms, such as tropical storms or hurricanes, depending on favorable atmospheric and oceanic conditions. Key factors influencing the development of TDs include sea surface temperature, wind shear, latent heat, moisture, ENSO phase and the Coriolis effect, which helps initiate the system's rotation.\n",
    "\n",
    "The current project focuses on the development of a machine learning-based forecast model designed to predict the future position, wind speed, and pressure of newly formed tropical depressions at 6-hour intervals. By leveraging historical tropical depression data and known ENSO phases, the model aims to predict the following:\n",
    "\n",
    "- **TD Track**: The latitudinal and longitudinal coordinates of the storm’s future location.\n",
    "- **Wind Speed**: Changes in the storm’s wind intensity.\n",
    "- **Minimum Central Pressure**: Evolution of the storm’s pressure, which is critical for determining its potential strength.\n",
    "- **Velocity and Direction**: Predict the movement speed and direction of the tropical depression.\n",
    "\n",
    "Furthermore, as the tropical depression develops, newly acquired data will be incorporated into the model to improve the accuracy of forecasting its further evolution.\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. The project aims to develope a series of machine learning models that will predict the future location, wind speed, pressure, and velocity of a tropical depression for the next 6, 12, 18, and 24 hours.\n",
    "2. Compare the accuracy of different ML models to determine which performs best across varying prediction intervals, with the expectation that the 6-hour prediction will be the most reliable and the 24-hour forecast less so.\n",
    "3. Evaluate the performance of the models in terms of bias, variance, and overall error using metrics like Mean Squared Error (MSE) and R-squared scores.\n",
    "4. Optimize the models through grid search and other tuning methods to ensure the best possible performance.\n",
    "\n",
    "### Steps to be Followed\n",
    "\n",
    "1. **Data Preparation and Feature Engineering**: The datasets from the previous project will be used. Some of the features will be transformed and scaled, others will be engineered.\n",
    "\n",
    "2. **Model Selection, Training and Testing**: The machine learning algorithms to be evaluated are Random Forest Regressor and Gradient Boosting Models (XGBoost). The dataset will be split into training and test sets, with cross-validation to ensure model robustness. Separate models will be trained for 6, 12, 18, and 24-hour prediction intervals, with the recursive use of previous predictions for longer intervals.\n",
    "\n",
    "4. **Model Evaluation**: The performance of the models will be evaluated using metrics such as MSE, R-squared, and bias-variance tradeoffs. The accuracy of the predictions will be analyzed, particularly regarding the 6-hour and 24-hour predictions, with an expectation of decreasing accuracy for longer intervals.\n",
    "\n",
    "5. **Optimization**: Hyperparameter tuning will be performed using grid search or randomized search to improve the models’ performance, especially in reducing prediction errors.\n",
    "\n",
    "## Data Preparation and Feature Engineering\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "The data used in this project comes from multiple reputable sources, ensuring comprehensive coverage of both tropical depression (TD) activity and environmental factors like sea surface temperature (SST) anomalies:\n",
    "\n",
    "The SST anomaly data was obtained from the NASA Earth Data AQUA MODIS satellite, providing high-resolution sea surface temperature measurements. These anomalies reflect deviations from normal sea temperatures, which are critical for understanding the effects of El Niño and La Niña events. The data is in .nc format\n",
    "\n",
    "Data for tropical depressions in the Northwest Pacific was sourced from the Japan Meteorological Agency (JMA). This dataset includes information on each tropical depression's intensity, minimum pressure, wind speeds, latitude, and longitude.\n",
    "\n",
    "The National Hurricane Center (NHC) provided tropical depression data for the Northeast and Central Pacific, which is similar to the JMA data in terms of recorded metrics like wind speed, pressure, and storm tracks.\n",
    "\n",
    "The ENSO phases (El Niño, La Niña, and Neutral) were derived from the ONI table provided by the Climate Prediction Center (CPC). The ONI is calculated as the rolling three-month mean SST anomaly in the Niño 3.4 region and is commonly used to identify ENSO phases.\n",
    "\n",
    "The GSHHS (Global Self-consistent, Hierarchical, High-resolution Shoreline Database) was also utilized in this project to provide accurate geographical boundaries and coastlines for visualizing tropical depression tracks. This dataset offers detailed representations of global shorelines, which are essential for plotting storm trajectories in relation to landmasses and understanding potential landfall locations. GSHHS data ensures that the geospatial analysis of tropical depressions remains accurate and visually consistent when overlaying storm tracks on global maps.\n",
    "\n",
    "The dataset from the previous project, which includes key information such as TD latitude, longitude, wind speed, minimum central pressure, and ENSO phase, will be further refined. Data cleaning steps included handling missing values, standardizing units, and aligning time formats. The full details of the data cleaning process can be found in the GitHub repository [here](https://github.com/StanDobrev11/enso_effect_on_npacific_tds), where the code for preprocessing is documented and accessible. This ensures the dataset is ready for further analysis and machine learning modeling.\n",
    "\n",
    "Remaining missing wind data for the JMA dataset will be generated using a Random Forest model, and similarly, missing pressure data for the NHC dataset will be completed using the same method. Additional derived features, such as velocity and direction, will be included, with necessary transformations like trigonometric calculations for geographical coordinates. Features such as latitude, longitude, ENSO phase (one-hot encoded), wind speed, pressure, and the derived velocity and direction will be used to train the models, ensuring a comprehensive representation of tropical depression dynamics.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "For data preparation, the function **prepare_dataframe()** is imported. Clear explanation of the transformation, performed on the dataframe, are described on the function's docstring. First, we will fill out the missing **wind** values from the JMA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6022890-0766-46c2-9eaf-a4bef282b658",
   "metadata": {},
   "source": [
    "# read jma data\n",
    "jma = pd.read_csv('data/csv_ready/jma_td.csv', index_col=0)\n",
    "\n",
    "# converting the longitude to standart values [-180:180]\n",
    "jma['lon'] = jma['lon'].apply(lambda x: 360 - x if x > 180 else x)\n",
    "\n",
    "# drop first row because first and second are same\n",
    "jma = jma.drop(jma.index[0])\n",
    "\n",
    "# add values for the wind, assuming minimum wind of the TD to be 35kn\n",
    "jma.loc[jma['min_pressure_mBar'] >= 980, 'max_wind_kn'] = 35"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a260af98-5c28-4542-b920-462805b6a064",
   "metadata": {},
   "source": [
    "jma = prepare_dataframe(jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e3a6db-7731-4676-92ec-51db2d2bd401",
   "metadata": {},
   "source": [
    "In this project, Cartesian coordinates are used to transform the geographical latitude and longitude into a format that is easier for machine learning models to process. Latitude and longitude represent curved surfaces on Earth, but Cartesian coordinates allow the model to work with a flat, linear representation, making it easier to capture spatial relationships in a way that ML algorithms can interpret effectively. This helps improve the accuracy of the predictions.\n",
    "\n",
    "Transforming the direction into sine and cosine components is done to avoid discontinuities that arise from using angles directly (since 0° and 360° are mathematically the same direction but numerically far apart). By using sine and cosine, the model can treat direction as a smooth, continuous feature, which better represents the cyclical nature of angles and helps improve learning efficiency.\n",
    "\n",
    "Plane Sailing is a simplified navigational technique used to estimate the course and distance between two points on Earth, assuming that the Earth is flat for small distances. This method works by treating the meridians (lines of longitude) and parallels (lines of latitude) as straight lines, which allows for easy calculations using basic trigonometry. Plane sailing assumes a flat Earth, which introduces only small errors when dealing with short distances. Over small distances, the curvature of the Earth is minimal and can be safely ignored. This makes plane sailing a convenient and fast method for navigation over distances typically less than 600 nautical miles (NM) and therefore, for calculating course and speed of TDs for time intervals of 6 hours.\n",
    "\n",
    "In the case of tropical depressions (TD), the speed and direction (or velocity and bearing) are relevant to describe the movement from one point to the next. When at point 0 (initial observation), we don't know the speed and direction until next observation position after 6 hours. Shifting the speed and direction by -1 ensures that at each point, the actual speed and direction required to reach the next point are used and the very last observation will have the speed and direction set to 0 to reflect TD's disipation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23474a6-3312-4f43-8b87-1236a92a0f13",
   "metadata": {},
   "source": [
    "jma[:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd08e2a-054d-40eb-a403-1e6643ce84ff",
   "metadata": {},
   "source": [
    "# the result of this line shows how many observation are missing wind value\n",
    "jma.max_wind_kn[jma.max_wind_kn == 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98c5c48-df5b-4c7d-af18-cbb1780deda8",
   "metadata": {},
   "source": [
    "# separate the dataframe\n",
    "available_data_jma = jma[jma['max_wind_kn'] > 0]\n",
    "predicted_data_jma = jma[jma['max_wind_kn'] == 0]\n",
    "\n",
    "X_raw_jma = available_data_jma[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw_jma = available_data_jma['max_wind_kn']\n",
    "\n",
    "# splitting the data 80/20 sounds reasonable. There are abt 62000 valid observations\n",
    "X_train_jma, X_test_jma, y_train_jma, y_test_jma = train_test_split(X_raw_jma, y_raw_jma, test_size=0.2, random_state=42)\n",
    "X_pred_jma = predicted_data_jma[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04c9a91-1b41-450c-9e46-b7b198ff2004",
   "metadata": {},
   "source": [
    "# building the pipeline\n",
    "preprocessor_jma = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['min_pressure_mBar', 'velocity_kn']),\n",
    "], remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "pipeline_jma = Pipeline([\n",
    "    ('preprocess', preprocessor_jma),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=97))\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fa0bd573-1f15-4a65-ab4d-7fdd2635efcf",
   "metadata": {},
   "source": [
    "Splitting the data 80/20 means using 80% of the dataset (about 49,600 observations) for training the model and the remaining 20% (about 12,400 observations) for testing. This is a common practice to ensure the model has enough data to learn from while still reserving a significant portion for evaluating its performance on unseen data. The train_test_split function with random_state=42 ensures that the split is reproducible, meaning the same split can be obtained each time the code is run.\n",
    "\n",
    "This pipeline automates the process of preparing the data and training the model for tropical depression forecasting.\n",
    "\n",
    "First, the preprocessor step handles the data transformations: **ENSO phases** (El Niño, La Niña, Neutral) are one-hot encoded to convert the categorical data into a format that the model can understand.\n",
    "**MinPressure** and **Velocity** are scaled using **Min-Max scaling** to normalize the values, making them easier for the model to interpret.\n",
    "Next, the **RandomForestRegressor** is applied to the preprocessed data. This model uses multiple decision trees to predict the next values for wind speed, pressure, and coordinates based on the historical data and ENSO phase. The pipeline combines all of these steps, making it easier to train and test the model efficiently.\n",
    "\n",
    "Then we fit the training data to the pipeline and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73051847-37d0-4efd-a2a2-a126c52d5a91",
   "metadata": {},
   "source": [
    "pipeline_jma.fit(X_train_jma, y_train_jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d630b-9511-40e7-ab17-f3662ec69b0c",
   "metadata": {},
   "source": [
    "pipeline_jma.score(X_test_jma, y_test_jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73903332-50fe-4441-a12a-68beabf53127",
   "metadata": {},
   "source": [
    "y_pred = pipeline_jma.predict(X_test_jma)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test_jma, y_pred)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9bf169fb-2442-426c-9376-91fd7a813c8f",
   "metadata": {},
   "source": [
    "The result shows that the RandomForestRegressor model is performing very well on the test data, with a high accuracy score of 0.99 (which indicates the model is correctly predicting 99% of the variance in the data). The Mean Squared Error (MSE) of 3.26 also indicates that, on average, the model's predictions deviate by about 3.26 units (kn) from the actual values. This is a low error value, further suggesting that the model is making precise predictions. The value of 3.26 kn deviation in the range of 35 - 140 kn of wind is acceptable.\n",
    "\n",
    "Further evaluation of the model's reliability and potential overfitting will not be conducted, as the model's output is intended solely for a one-time use to fill missing data in the dataset.\n",
    "\n",
    "With that in mind, missing wind data will be now applied to the jma dataset and will check if there are any '0' values remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176a3308-db4a-4cfa-af69-966c2df1759d",
   "metadata": {},
   "source": [
    "predicted_winds = pipeline_jma.predict(X_pred_jma)\n",
    "jma.loc[jma['max_wind_kn'] == 0, 'max_wind_kn'] = predicted_winds.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e20068b-31db-4a81-9c7b-b59a68bbe599",
   "metadata": {},
   "source": [
    "jma.max_wind_kn[jma.max_wind_kn == 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fb17af-f610-4213-b8ee-7a1d6ed927fc",
   "metadata": {},
   "source": [
    "importances = pipeline_jma.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline_jma.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f39faddd-acec-444b-b9b5-5ca232582fc0",
   "metadata": {},
   "source": [
    "The feature importance table shows how much each feature contributes to the model's predictive power. In this case, min_pressure_mBar is the most important feature by far, with a feature importance score of 0.986759. This suggests that the model relies heavily on the minimum central pressure to make accurate predictions. The remaining features, including the x, y, z cartesian coordinates, velocity_kn, direction_sin, direction_cos, and the ENSO phase encoded as one-hot variables, have much lower importance, indicating that their contribution to the model's performance is minimal in comparison. This suggests that the pressure plays a critical role in determining the outcome, while the other features provide only marginal improvements.\n",
    "\n",
    "A similar operation will be performed on the NHC dataset; however, in this case, the missing values are for minimum central pressure. The pressure values that are to be modeled, have a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53295a31-e4e1-4eeb-8154-57208e43290a",
   "metadata": {},
   "source": [
    "nhc = pd.read_csv('data/csv_ready/ne_pacific_td.csv', index_col=0)\n",
    "# converting the longitude to standart values [-180:180]\n",
    "nhc['lon'] = 180 - nhc['lon']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936638bc-fcbb-41c4-abf5-a8e99cb63d31",
   "metadata": {},
   "source": [
    "nhc = prepare_dataframe(nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b55c528-e46e-4b7f-b952-39680c131941",
   "metadata": {},
   "source": [
    "# building the pipeline\n",
    "preprocessor_nhc = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['max_wind_kn', 'velocity_kn']),\n",
    "], remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "pipeline_nhc = Pipeline([\n",
    "    ('preprocess', preprocessor_nhc),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=97))\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ea570-a74d-4275-afed-defd37e01007",
   "metadata": {},
   "source": [
    "# separate the dataframe\n",
    "available_data_nhc = nhc[nhc['min_pressure_mBar'] > 0]\n",
    "predicted_data_nhc = nhc[nhc['min_pressure_mBar'] < 0]\n",
    "\n",
    "X_raw_nhc = available_data_nhc[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw_nhc = available_data_nhc['min_pressure_mBar']\n",
    "\n",
    "# splitting the data 80/20 sounds reasonable. There are abt 62000 valid observations\n",
    "X_train_nhc, X_test_nhc, y_train_nhc, y_test_nhc = train_test_split(X_raw_nhc, y_raw_nhc, test_size=0.2, random_state=97)\n",
    "X_pred_nhc = predicted_data_nhc[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d881e-efe1-474c-ab1e-c7feb20dcc82",
   "metadata": {},
   "source": [
    "pipeline_nhc.fit(X_train_nhc, y_train_nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06991e9a-9557-4eb1-8894-c9fba5fe72bd",
   "metadata": {},
   "source": [
    "pipeline_nhc.score(X_test_nhc, y_test_nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408186d9-bf86-47ad-9000-777760e0e93f",
   "metadata": {},
   "source": [
    "y_pred = pipeline_nhc.predict(X_test_nhc)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test_nhc, y_pred)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce952f0f-cbdb-4890-ac5d-7af946bddd06",
   "metadata": {},
   "source": [
    "predicted_pressure = pipeline_nhc.predict(X_pred_nhc)\n",
    "nhc.loc[nhc['min_pressure_mBar'] < 0, 'min_pressure_mBar'] = predicted_pressure.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acbe6db-158e-4e67-9bc3-1085379f5f4e",
   "metadata": {},
   "source": [
    "nhc.min_pressure_mBar[nhc.min_pressure_mBar < 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07bbe6c-7dc2-45f3-ac89-917944cc3256",
   "metadata": {},
   "source": [
    "importances = pipeline_nhc.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline_nhc.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "95c75659-698e-47dc-84cd-723bbaaa9d8a",
   "metadata": {},
   "source": [
    "Now that the missing values for both datasets, JMA and NHC are completed, they are saved for further usage in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd0c4eb-f09c-4102-bbe6-bb9b910016be",
   "metadata": {},
   "source": [
    "# dropping columns not required and saving the datasets\n",
    "jma = jma.drop(columns=['direction_sin', 'direction_cos', 'x', 'y', 'z'])\n",
    "nhc = nhc.drop(columns=['direction_sin', 'direction_cos', 'x', 'y', 'z'])\n",
    "jma.to_csv('data/csv_ready/jma_training')\n",
    "nhc.to_csv('data/csv_ready/nhc_training')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "988526cb-d7f7-42b8-904b-5b8ff951ac84",
   "metadata": {},
   "source": [
    "## Model Selection, Training & Testing\n",
    "\n",
    "### Conceptual Clarification of Prediction Setup\n",
    "The idea is to predict the next position, speed, and course based on how the storm reached the current position. For this:\n",
    "\n",
    "**Training Data Setup:**\n",
    "\n",
    "- Each row should include information on the storm's movement (speed and course) toward the current position as lagged features.\n",
    "- The target variables (latitude, longitude, wind speed and minimum pressure) represent the next position's attributes.\n",
    "Prediction:\n",
    "- Input the current position and movement characteristics (speed, course, and lagged features) to predict the attributes of the next position.\n",
    "\n",
    "The add_lags function creates lagged features by shifting data for specific columns grouped by storms (e.g., grouped by a unique identifier like storm_id). However, to make it more precise, the function is deriving speed and course directly from latitude/longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f947d4-34d4-4182-94dd-a3767d219728",
   "metadata": {},
   "source": [
    "def add_lags(df, features, n_lags, group_col):\n",
    "    \"\"\"\n",
    "    Adds lagged features for time series grouped by a specific column.\n",
    "\n",
    "    Args:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - features (list): List of feature names to lag.\n",
    "    - n_lags (int): Number of lag periods.\n",
    "    - group_col (str): Column to group by (e.g., 'storm_id').\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: The DataFrame with lagged features added.\n",
    "    \"\"\"\n",
    "    for lag in range(1, n_lags + 1):\n",
    "        for feature in features:\n",
    "            df[f\"{feature}_lag_{lag}\"] = df.groupby(group_col)[feature].shift(lag)\n",
    "    # Drop rows where lagged values are NaN due to shifting\n",
    "    return df.dropna(subset=[f\"{feature}_lag_{lag}\" for feature in features])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "172ef168-7e6a-4988-a540-fbde144d52ae",
   "metadata": {},
   "source": [
    "### XGBRegressor Workflow and Adaptation for Time Series Prediction\n",
    "\n",
    "XGBRegressor is incorporated into the project for its ability to model complex, non-linear relationships efficiently while maintaining robustness and scalability. Unlike linear models like SGDRegressor, XGBRegressor excels at handling structured time-series data by leveraging gradient boosting techniques to optimize predictions iteratively. This makes it a strong candidate for modeling tropical depression (TD) tracks and intensities.\n",
    "\n",
    "For this project, XGBRegressor provides the flexibility to handle mixed data types (e.g., categorical and continuous features like ENSO phases, wind speed, latitude, and longitude) and allows for advanced feature engineering and custom loss functions. Its tree-based nature eliminates the need for extensive feature scaling, simplifying the preprocessing pipeline.\n",
    "\n",
    "#### Workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "873c064d-2488-44a0-92d8-6918743a5f19",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3ce98a7-6723-40f5-9ba8-02aa7ae59b96",
   "metadata": {},
   "source": [
    "prep_data = FunctionTransformer(func=prepare_dataframe)\n",
    "prep_train_data = FunctionTransformer(func=generate_training_dataframe)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ec4d6bcb-c6cd-4b2b-affb-5b7e7a256925",
   "metadata": {},
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),  # One-hot encode the ENSO feature\n",
    "        ('poly', PolynomialFeatures(degree=3, include_bias=False), slice(0, None)),\n",
    "    ], remainder='passthrough', force_int_remainder_cols=False)\n",
    "scaler = MinMaxScaler()\n",
    "regressor = MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=3000, penalty='l1', alpha=1e-6, tol=1e-5, random_state=97))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04fb7ece-fe8f-4251-907b-24c5e6114e42",
   "metadata": {},
   "source": [
    "jma_pred_pipeline_sgd = Pipeline([\n",
    "        ('prep_data', prep_data),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "jma_train_pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bdba7cbc-baef-4b46-9389-1bf6ddd0267c",
   "metadata": {},
   "source": [
    "# df_trn = prep_data.transform(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0cdc43d4-5a1f-43ed-b1d8-2a782e02d9ea",
   "metadata": {},
   "source": [
    "# df_trn = prep_train_data.transform(df_trn)\n",
    "df_trn = pd.read_csv('data/csv_ready/df_trn.csv')\n",
    "X_train, X_test, y_train, y_test = next(split_dataframe(df_trn, splitter='gss'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620c503b-b83b-45b8-b575-14b17b76b670",
   "metadata": {},
   "source": [
    "jma_train_pipeline_sgd.fit(X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "356ee3e3-1c17-4877-a60d-b2008eba1719",
   "metadata": {},
   "source": [
    "jma_train_pipeline_sgd.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f55f600-3afc-4720-a326-ac99dc3ead0d",
   "metadata": {},
   "source": [
    "X_test.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38bcad5f-3e85-4acd-ba56-3cb9258b55c6",
   "metadata": {},
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d84246c4-04ae-4a38-a5ed-2e10ab3224be",
   "metadata": {},
   "source": [
    "plot_tds(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1dfc108-0e3d-4705-bfd4-f9f8b80e63ef",
   "metadata": {},
   "source": [
    "y_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf49f8ae-d314-445a-96f0-72b82b8693ca",
   "metadata": {},
   "source": [
    "X_pred = jma[jma.index == '2023-10-14 18:00:00']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cffae58-eecd-4ab1-bca6-aad36f003c63",
   "metadata": {},
   "source": [
    "X_expected = jma[jma.index == '2023-10-15 00:00:00']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6a65c7b-a3da-479e-a4fa-d1329b70def3",
   "metadata": {},
   "source": [
    "X_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85ca3e34-da88-40ee-b36f-d2659bc532fe",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83f014b6-227d-401c-bfe2-6f3b8c156c11",
   "metadata": {},
   "source": [
    "jma_pred_pipeline_sgd.predict(X_pred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e5e1764-edfa-4c54-8f9e-d370109deff4",
   "metadata": {},
   "source": [
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf9e7aa-49cf-4556-8900-f66569e3b694",
   "metadata": {},
   "source": [
    "df_ = create_real_pred_df(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9aa8f0-8794-4316-9ca6-b2ccf69b052d",
   "metadata": {},
   "source": [
    "**Method to Calculate Probability of Predicted Position**\n",
    "\n",
    "To calculate the probability of the predicted position being within a certain range of the actual position, we can use a statistical approach based on error distributions. The error distribution is calculated between predicted and real positions for the test dataset. Then we fit a normal distribution to the error values in terms of latitude and longitude differences. The standard deviation (σ) of the error distribution is used to estimate the confidence interval.\n",
    "\n",
    "Using the fitted distribution, we calculate the probability that the actual position falls within a certain radius of the predicted position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b1ccb-07ca-4c98-a3ba-6da56b07fa5c",
   "metadata": {},
   "source": [
    "df_= probability_within_radius(df_, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a8a3f4-4187-4d40-ae96-1d7a95decdeb",
   "metadata": {},
   "source": [
    "df_.probability_within_radius.min(), df_.probability_within_radius.max()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ead7e-7550-4073-ae62-3b3550df65a3",
   "metadata": {},
   "source": [
    "df_[df_.group == 127]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b016f0-12f9-4280-8d9f-c01768ccea88",
   "metadata": {},
   "source": [
    "X_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612de00-0cd5-4470-b6b0-88d7c8e372c8",
   "metadata": {},
   "source": [
    "X_test[X_test.velocity_kn > 15][30:50]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f5ed8-052c-422e-87ef-a19dadd1cfd6",
   "metadata": {},
   "source": [
    "X_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd3368f-2917-4097-b537-7c12d63793a6",
   "metadata": {},
   "source": [
    "X_pred = jma[jma.index == '1952-10-11 12:00:00']\n",
    "X_expected = jma[jma.index == '1952-10-11 18:00:00']\n",
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e0f556-b18d-4522-85f1-6091006e92b7",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
