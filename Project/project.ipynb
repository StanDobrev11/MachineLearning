{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "source": [
    "%matplotlib inline"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1362a891-720b-48bd-a322-a49cc52be301",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import prepare_dataframe, generate_training_dataframe, split_dataframe, manage_prediction, create_real_pred_df, plot_tds, probability_within_radius\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, FunctionTransformer, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from scipy.stats import norm"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9de97d0d-1c25-408c-add1-7390573ade48",
   "metadata": {},
   "source": [
    "# North Pacific Ocean Tropical Depressions Novel Forecast Model\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Building upon the foundational [analysis of the effects of the El Niño-Southern Oscillation (ENSO) on tropical depressions (TDs) in the North Pacific Ocean](https://github.com/StanDobrev11/enso_effect_on_npacific_tds), this current project takes the research a step further by developing a machine learning (ML) forecast model. The aim is to predict the track, intensity, and progression of tropical depressions based on their initial characteristics and ENSO phases.\n",
    "\n",
    "In the previous project, correlations between ENSO phases and various tropical depression characteristics, such as frequency, intensity, and storm tracks were successfully identified, especially in the NE Pacific region. It was established that El Niño and La Niña events have discernible impacts on where and how frequently TDs form, as well as their overall intensity across the North Pacific region. This prior work provided valuable insights into how different ENSO conditions influence tropical depressions, laying a strong foundation for more advanced predictive modeling.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "### Brief explanation of ENSO and TDs\n",
    "\n",
    "The **El Niño-Southern Oscillation (ENSO)** is a climate phenomenon characterized by fluctuations in sea surface temperatures and atmospheric conditions in the Pacific Ocean. ENSO consists of three phases: **El Niño**, when ocean waters in the central and eastern Pacific are warmer than average; **La Niña**, when these waters are cooler than normal; and a **Neutral** phase when conditions are closer to the long-term average. ENSO has significant global impacts, affecting weather patterns, rainfall, and storm activity.\n",
    "\n",
    "A **Tropical Depression (TD)** is a low-pressure weather system that typically develops around 5 degrees north or south of the equator. These systems are the early stage of tropical cyclones and can intensify into stronger storms, such as tropical storms or hurricanes, depending on favorable atmospheric and oceanic conditions. Key factors influencing the development of TDs include sea surface temperature, wind shear, latent heat, moisture, ENSO phase and the Coriolis effect, which helps initiate the system's rotation.\n",
    "\n",
    "The current project focuses on the development of a machine learning-based forecast model designed to predict the future position, wind speed, and pressure of newly formed tropical depressions at 6-hour intervals. By leveraging historical tropical depression data and known ENSO phases, the model aims to predict the following:\n",
    "\n",
    "- **TD Track**: The latitudinal and longitudinal coordinates of the storm’s future location.\n",
    "- **Wind Speed**: Changes in the storm’s wind intensity.\n",
    "- **Minimum Central Pressure**: Evolution of the storm’s pressure, which is critical for determining its potential strength.\n",
    "- **Velocity and Direction**: Predict the movement speed and direction of the tropical depression.\n",
    "\n",
    "Furthermore, as the tropical depression develops, newly acquired data will be incorporated into the model to improve the accuracy of forecasting its further evolution.\n",
    "\n",
    "### Goals\n",
    "\n",
    "1. The project aims to develope a series of machine learning models that will predict the future location, wind speed, pressure, and velocity of a tropical depression for the next 6, 12, 18, and 24 hours.\n",
    "2. Compare the accuracy of different ML models to determine which performs best across varying prediction intervals, with the expectation that the 6-hour prediction will be the most reliable and the 24-hour forecast less so.\n",
    "3. Evaluate the performance of the models in terms of bias, variance, and overall error using metrics like Mean Squared Error (MSE) and R-squared scores.\n",
    "4. Optimize the models through grid search and other tuning methods to ensure the best possible performance.\n",
    "\n",
    "### Steps to be Followed\n",
    "\n",
    "1. **Data Preparation and Feature Engineering**: The datasets from the previous project will be used. Some of the features will be transformed and scaled, others will be engineered.\n",
    "\n",
    "2. **Model Selection, Training and Testing**: The machine learning algorithms to be evaluated are Random Forest Regressor and Gradient Boosting Models (XGBoost). The dataset will be split into training and test sets, with cross-validation to ensure model robustness. Separate models will be trained for 6, 12, 18, and 24-hour prediction intervals, with the recursive use of previous predictions for longer intervals.\n",
    "\n",
    "4. **Model Evaluation**: The performance of the models will be evaluated using metrics such as MSE, R-squared, and bias-variance tradeoffs. The accuracy of the predictions will be analyzed, particularly regarding the 6-hour and 24-hour predictions, with an expectation of decreasing accuracy for longer intervals.\n",
    "\n",
    "5. **Optimization**: Hyperparameter tuning will be performed using grid search or randomized search to improve the models’ performance, especially in reducing prediction errors.\n",
    "\n",
    "## Data Preparation and Feature Engineering\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "The data used in this project comes from multiple reputable sources, ensuring comprehensive coverage of both tropical depression (TD) activity and environmental factors like sea surface temperature (SST) anomalies:\n",
    "\n",
    "The SST anomaly data was obtained from the NASA Earth Data AQUA MODIS satellite, providing high-resolution sea surface temperature measurements. These anomalies reflect deviations from normal sea temperatures, which are critical for understanding the effects of El Niño and La Niña events. The data is in .nc format\n",
    "\n",
    "Data for tropical depressions in the Northwest Pacific was sourced from the Japan Meteorological Agency (JMA). This dataset includes information on each tropical depression's intensity, minimum pressure, wind speeds, latitude, and longitude.\n",
    "\n",
    "The National Hurricane Center (NHC) provided tropical depression data for the Northeast and Central Pacific, which is similar to the JMA data in terms of recorded metrics like wind speed, pressure, and storm tracks.\n",
    "\n",
    "The ENSO phases (El Niño, La Niña, and Neutral) were derived from the ONI table provided by the Climate Prediction Center (CPC). The ONI is calculated as the rolling three-month mean SST anomaly in the Niño 3.4 region and is commonly used to identify ENSO phases.\n",
    "\n",
    "The GSHHS (Global Self-consistent, Hierarchical, High-resolution Shoreline Database) was also utilized in this project to provide accurate geographical boundaries and coastlines for visualizing tropical depression tracks. This dataset offers detailed representations of global shorelines, which are essential for plotting storm trajectories in relation to landmasses and understanding potential landfall locations. GSHHS data ensures that the geospatial analysis of tropical depressions remains accurate and visually consistent when overlaying storm tracks on global maps.\n",
    "\n",
    "The dataset from the previous project, which includes key information such as TD latitude, longitude, wind speed, minimum central pressure, and ENSO phase, will be further refined. Data cleaning steps included handling missing values, standardizing units, and aligning time formats. The full details of the data cleaning process can be found in the GitHub repository [here](https://github.com/StanDobrev11/enso_effect_on_npacific_tds), where the code for preprocessing is documented and accessible. This ensures the dataset is ready for further analysis and machine learning modeling.\n",
    "\n",
    "Remaining missing wind data for the JMA dataset will be generated using a Random Forest model, and similarly, missing pressure data for the NHC dataset will be completed using the same method. Additional derived features, such as velocity and direction, will be included, with necessary transformations like trigonometric calculations for geographical coordinates. Features such as latitude, longitude, ENSO phase (one-hot encoded), wind speed, pressure, and the derived velocity and direction will be used to train the models, ensuring a comprehensive representation of tropical depression dynamics.\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "For data preparation, the function **prepare_dataframe()** is imported. Clear explanation of the transformation, performed on the dataframe, are described on the function's docstring. First, we will fill out the missing **wind** values from the JMA dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6022890-0766-46c2-9eaf-a4bef282b658",
   "metadata": {},
   "source": [
    "# read jma data\n",
    "jma = pd.read_csv('data/csv_ready/jma_td.csv', index_col=0)\n",
    "\n",
    "# converting the longitude to standart values [-180:180]\n",
    "jma['lon'] = jma['lon'].apply(lambda x: 360 - x if x > 180 else x)\n",
    "\n",
    "# drop first row because first and second are same\n",
    "jma = jma.drop(jma.index[0])\n",
    "\n",
    "# add values for the wind, assuming minimum wind of the TD to be 35kn\n",
    "jma.loc[jma['min_pressure_mBar'] >= 980, 'max_wind_kn'] = 35"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a260af98-5c28-4542-b920-462805b6a064",
   "metadata": {},
   "source": [
    "jma = prepare_dataframe(jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e3a6db-7731-4676-92ec-51db2d2bd401",
   "metadata": {},
   "source": [
    "In this project, Cartesian coordinates are used to transform the geographical latitude and longitude into a format that is easier for machine learning models to process. Latitude and longitude represent curved surfaces on Earth, but Cartesian coordinates allow the model to work with a flat, linear representation, making it easier to capture spatial relationships in a way that ML algorithms can interpret effectively. This helps improve the accuracy of the predictions.\n",
    "\n",
    "Transforming the direction into sine and cosine components is done to avoid discontinuities that arise from using angles directly (since 0° and 360° are mathematically the same direction but numerically far apart). By using sine and cosine, the model can treat direction as a smooth, continuous feature, which better represents the cyclical nature of angles and helps improve learning efficiency.\n",
    "\n",
    "Plane Sailing is a simplified navigational technique used to estimate the course and distance between two points on Earth, assuming that the Earth is flat for small distances. This method works by treating the meridians (lines of longitude) and parallels (lines of latitude) as straight lines, which allows for easy calculations using basic trigonometry. Plane sailing assumes a flat Earth, which introduces only small errors when dealing with short distances. Over small distances, the curvature of the Earth is minimal and can be safely ignored. This makes plane sailing a convenient and fast method for navigation over distances typically less than 600 nautical miles (NM) and therefore, for calculating course and speed of TDs for time intervals of 6 hours.\n",
    "\n",
    "In the case of tropical depressions (TD), the speed and direction (or velocity and bearing) are relevant to describe the movement from one point to the next. When at point 0 (initial observation), we don't know the speed and direction until next observation position after 6 hours. Shifting the speed and direction by -1 ensures that at each point, the actual speed and direction required to reach the next point are used and the very last observation will have the speed and direction set to 0 to reflect TD's disipation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a23474a6-3312-4f43-8b87-1236a92a0f13",
   "metadata": {},
   "source": [
    "jma[:10]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3dd08e2a-054d-40eb-a403-1e6643ce84ff",
   "metadata": {},
   "source": [
    "# the result of this line shows how many observation are missing wind value\n",
    "jma.max_wind_kn[jma.max_wind_kn == 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d98c5c48-df5b-4c7d-af18-cbb1780deda8",
   "metadata": {},
   "source": [
    "# separate the dataframe\n",
    "available_data_jma = jma[jma['max_wind_kn'] > 0]\n",
    "predicted_data_jma = jma[jma['max_wind_kn'] == 0]\n",
    "\n",
    "X_raw_jma = available_data_jma[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw_jma = available_data_jma['max_wind_kn']\n",
    "\n",
    "# splitting the data 80/20 sounds reasonable. There are abt 62000 valid observations\n",
    "X_train_jma, X_test_jma, y_train_jma, y_test_jma = train_test_split(X_raw_jma, y_raw_jma, test_size=0.2, random_state=42)\n",
    "X_pred_jma = predicted_data_jma[['min_pressure_mBar', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f04c9a91-1b41-450c-9e46-b7b198ff2004",
   "metadata": {},
   "source": [
    "# building the pipeline\n",
    "preprocessor_jma = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['min_pressure_mBar', 'velocity_kn']),\n",
    "], remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "pipeline_jma = Pipeline([\n",
    "    ('preprocess', preprocessor_jma),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=97))\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fa0bd573-1f15-4a65-ab4d-7fdd2635efcf",
   "metadata": {},
   "source": [
    "Splitting the data 80/20 means using 80% of the dataset (about 49,600 observations) for training the model and the remaining 20% (about 12,400 observations) for testing. This is a common practice to ensure the model has enough data to learn from while still reserving a significant portion for evaluating its performance on unseen data. The train_test_split function with random_state=42 ensures that the split is reproducible, meaning the same split can be obtained each time the code is run.\n",
    "\n",
    "This pipeline automates the process of preparing the data and training the model for tropical depression forecasting.\n",
    "\n",
    "First, the preprocessor step handles the data transformations: **ENSO phases** (El Niño, La Niña, Neutral) are one-hot encoded to convert the categorical data into a format that the model can understand.\n",
    "**MinPressure** and **Velocity** are scaled using **Min-Max scaling** to normalize the values, making them easier for the model to interpret.\n",
    "Next, the **RandomForestRegressor** is applied to the preprocessed data. This model uses multiple decision trees to predict the next values for wind speed, pressure, and coordinates based on the historical data and ENSO phase. The pipeline combines all of these steps, making it easier to train and test the model efficiently.\n",
    "\n",
    "Then we fit the training data to the pipeline and evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73051847-37d0-4efd-a2a2-a126c52d5a91",
   "metadata": {},
   "source": [
    "pipeline_jma.fit(X_train_jma, y_train_jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a82d630b-9511-40e7-ab17-f3662ec69b0c",
   "metadata": {},
   "source": [
    "pipeline_jma.score(X_test_jma, y_test_jma)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "73903332-50fe-4441-a12a-68beabf53127",
   "metadata": {},
   "source": [
    "y_pred = pipeline_jma.predict(X_test_jma)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test_jma, y_pred)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "9bf169fb-2442-426c-9376-91fd7a813c8f",
   "metadata": {},
   "source": [
    "The result shows that the RandomForestRegressor model is performing very well on the test data, with a high accuracy score of 0.99 (which indicates the model is correctly predicting 99% of the variance in the data). The Mean Squared Error (MSE) of 3.26 also indicates that, on average, the model's predictions deviate by about 3.26 units (kn) from the actual values. This is a low error value, further suggesting that the model is making precise predictions. The value of 3.26 kn deviation in the range of 35 - 140 kn of wind is acceptable.\n",
    "\n",
    "Further evaluation of the model's reliability and potential overfitting will not be conducted, as the model's output is intended solely for a one-time use to fill missing data in the dataset.\n",
    "\n",
    "With that in mind, missing wind data will be now applied to the jma dataset and will check if there are any '0' values remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "176a3308-db4a-4cfa-af69-966c2df1759d",
   "metadata": {},
   "source": [
    "predicted_winds = pipeline_jma.predict(X_pred_jma)\n",
    "jma.loc[jma['max_wind_kn'] == 0, 'max_wind_kn'] = predicted_winds.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e20068b-31db-4a81-9c7b-b59a68bbe599",
   "metadata": {},
   "source": [
    "jma.max_wind_kn[jma.max_wind_kn == 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42fb17af-f610-4213-b8ee-7a1d6ed927fc",
   "metadata": {},
   "source": [
    "importances = pipeline_jma.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline_jma.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f39faddd-acec-444b-b9b5-5ca232582fc0",
   "metadata": {},
   "source": [
    "The feature importance table shows how much each feature contributes to the model's predictive power. In this case, min_pressure_mBar is the most important feature by far, with a feature importance score of 0.986759. This suggests that the model relies heavily on the minimum central pressure to make accurate predictions. The remaining features, including the x, y, z cartesian coordinates, velocity_kn, direction_sin, direction_cos, and the ENSO phase encoded as one-hot variables, have much lower importance, indicating that their contribution to the model's performance is minimal in comparison. This suggests that the pressure plays a critical role in determining the outcome, while the other features provide only marginal improvements.\n",
    "\n",
    "A similar operation will be performed on the NHC dataset; however, in this case, the missing values are for minimum central pressure. The pressure values that are to be modeled, have a negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "53295a31-e4e1-4eeb-8154-57208e43290a",
   "metadata": {},
   "source": [
    "nhc = pd.read_csv('data/csv_ready/ne_pacific_td.csv', index_col=0)\n",
    "# converting the longitude to standart values [-180:180]\n",
    "nhc['lon'] = 180 - nhc['lon']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "936638bc-fcbb-41c4-abf5-a8e99cb63d31",
   "metadata": {},
   "source": [
    "nhc = prepare_dataframe(nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b55c528-e46e-4b7f-b952-39680c131941",
   "metadata": {},
   "source": [
    "# building the pipeline\n",
    "preprocessor_nhc = ColumnTransformer([\n",
    "    ('enso', OneHotEncoder(), ['enso']),\n",
    "    ('scaler', MinMaxScaler(), ['max_wind_kn', 'velocity_kn']),\n",
    "], remainder='passthrough', force_int_remainder_cols=False)\n",
    "\n",
    "pipeline_nhc = Pipeline([\n",
    "    ('preprocess', preprocessor_nhc),  # Apply scaling and encoding\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=97))\n",
    "])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "044ea570-a74d-4275-afed-defd37e01007",
   "metadata": {},
   "source": [
    "# separate the dataframe\n",
    "available_data_nhc = nhc[nhc['min_pressure_mBar'] > 0]\n",
    "predicted_data_nhc = nhc[nhc['min_pressure_mBar'] < 0]\n",
    "\n",
    "X_raw_nhc = available_data_nhc[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]\n",
    "y_raw_nhc = available_data_nhc['min_pressure_mBar']\n",
    "\n",
    "# splitting the data 80/20 sounds reasonable. There are abt 62000 valid observations\n",
    "X_train_nhc, X_test_nhc, y_train_nhc, y_test_nhc = train_test_split(X_raw_nhc, y_raw_nhc, test_size=0.2, random_state=97)\n",
    "X_pred_nhc = predicted_data_nhc[['max_wind_kn', 'velocity_kn', 'direction_sin', 'direction_cos', 'x', 'y', 'z', 'enso']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "717d881e-efe1-474c-ab1e-c7feb20dcc82",
   "metadata": {},
   "source": [
    "pipeline_nhc.fit(X_train_nhc, y_train_nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06991e9a-9557-4eb1-8894-c9fba5fe72bd",
   "metadata": {},
   "source": [
    "pipeline_nhc.score(X_test_nhc, y_test_nhc)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "408186d9-bf86-47ad-9000-777760e0e93f",
   "metadata": {},
   "source": [
    "y_pred = pipeline_nhc.predict(X_test_nhc)\n",
    "print(f'Mean Squared Error: {mean_squared_error(y_test_nhc, y_pred)}')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce952f0f-cbdb-4890-ac5d-7af946bddd06",
   "metadata": {},
   "source": [
    "predicted_pressure = pipeline_nhc.predict(X_pred_nhc)\n",
    "nhc.loc[nhc['min_pressure_mBar'] < 0, 'min_pressure_mBar'] = predicted_pressure.astype(int)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4acbe6db-158e-4e67-9bc3-1085379f5f4e",
   "metadata": {},
   "source": [
    "nhc.min_pressure_mBar[nhc.min_pressure_mBar < 0].count()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e07bbe6c-7dc2-45f3-ac89-917944cc3256",
   "metadata": {},
   "source": [
    "importances = pipeline_nhc.named_steps['regressor'].feature_importances_\n",
    "feature_names = pipeline_nhc.named_steps['preprocess'].get_feature_names_out()\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "print(feature_importance_df.sort_values(by='Importance', ascending=False))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "95c75659-698e-47dc-84cd-723bbaaa9d8a",
   "metadata": {},
   "source": [
    "Now that the missing values for both datasets, JMA and NHC are completed, they are saved for further usage in ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecd0c4eb-f09c-4102-bbe6-bb9b910016be",
   "metadata": {},
   "source": [
    "# dropping columns not required and saving the datasets\n",
    "jma = jma.drop(columns=['direction_sin', 'direction_cos', 'x', 'y', 'z'])\n",
    "nhc = nhc.drop(columns=['direction_sin', 'direction_cos', 'x', 'y', 'z'])\n",
    "jma.to_csv('data/csv_ready/jma_training')\n",
    "nhc.to_csv('data/csv_ready/nhc_training')"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "988526cb-d7f7-42b8-904b-5b8ff951ac84",
   "metadata": {},
   "source": [
    "## Model Selection, Training & Testing\n",
    "\n",
    "In the project, Random Forest, Gradient Boosting (XGBoost/LightGBM), and Stochastic Gradient Descent (SGDRegressor) are selected for time series prediction of tropical depression (TD) tracks and intensity due to their strong performance in handling non-linear relationships and structured time series data. These models are well-suited for working with both categorical and continuous features, such as ENSO phase, latitude, longitude, wind speed, and pressure, which are key components of this study.\n",
    "\n",
    "**How the Training Is Set Up**:\n",
    "\n",
    "For each observation, the speed and course (direction) were calculated based on the previous position using plain sailing.\n",
    "Then, these speed and course values were shifted to the previous row, so the course and speed reflect how the storm moved to the current position from the previous one.\n",
    "\n",
    "For example:\n",
    "```\n",
    "1951-05-03 18:00:00\t4\tIRI\t11.4\t130.4\t104\t909\t-1\t13.4\t307.0\n",
    "1951-05-04 00:00:00\t4\tIRI\t12.2\t129.3\t110\t910\t-1\t15.0\t258.0\n",
    "1951-05-04 06:00:00\t4\tIRI\t11.9\t127.8\t109\t909\t-1\t10.6\t248.0\n",
    "```\n",
    "In the observation at 1951-05-03 18:00:00, the speed is 13.4 knots and the course 307.0 degrees.\n",
    "This is the speed and course that the storm took to get to the position on 1951-05-04 00:00:00.\n",
    "\n",
    "The next observation at 1951-05-04 00:00:00 shows a new speed and course for the next 6-hour movement.\n",
    "\n",
    "**During Prediction**:\n",
    "\n",
    "When we predict for the next position (e.g., 1951-05-04 06:00:00), the current speed and course represent how the storm arrived at the current position.\n",
    "\n",
    "So, for prediction, the input should include the speed and course that correspond to the movement toward the current position, because we are essentially asking the model: \"Given that the storm traveled to the current position at this speed and course, where will it be in the next 6 hours?\"\n",
    "\n",
    "In this case, the data for prediction should look like this:\n",
    "```\n",
    "1951-05-03 18:00:00\t4\tIRI\t11.4\t130.4\t104\t909\t-1\t0       0\n",
    "1951-05-04 00:00:00\t4\tIRI\t12.2\t129.3\t110\t910\t-1\t13.4\t307.0\n",
    "1951-05-04 06:00:00\t4\tIRI\t11.9\t127.8\t109\t909\t-1\t15.0\t258.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d5f9e-1e0d-490e-a29b-71cfd1c194cd",
   "metadata": {},
   "source": [
    "### Random Forest for Time Series\n",
    "\n",
    "Random Forest is ideal for this project due to its robustness in capturing non-linear relationships between multiple variables like ENSO phases, wind speed, pressure, and geospatial data (latitude and longitude). By incorporating lagged features (previous time steps of lat/lon, wind speed, pressure), Random Forest can predict future locations and intensities of tropical depressions with high accuracy. Additionally, its ability to provide interpretable results through feature importance will allow us to understand which factors are most critical in influencing TD development and movement.\n",
    "\n",
    "However, **Random Forest** requires careful feature engineering, such as generating lag features and applying trigonometric transformations to latitude/longitude to maintain spatial consistency. Transformation of geographical coordinates (latitude and longitude) and direction into sine and cosine components is necessary to avoid treating these as independent variables.\n",
    "\n",
    "### Gradient Boosting (XGBoost/LightGBM)\n",
    "\n",
    "Gradient Boosting algorithms, particularly XGBoost and LightGBM, offer higher accuracy and efficiency, especially with larger datasets. Given the structured and complex nature of the time series data in this project, including multiple features like wind, pressure, velocity, and geographic location, Gradient Boosting is a powerful method for handling this complexity. It is particularly useful when combined with careful feature engineering, such as lagged features and trigonometric transformations for geographic data.\n",
    "\n",
    "However, **XGBoost and LightGBM** require more intensive hyperparameter tuning compared to Random Forest, which can be time-consuming but necessary to achieve optimal results. Similar to Random Forest, these models also require careful engineering of lagged features and transformations for latitude and longitude to capture the spatial relationships of tropical depressions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172ef168-7e6a-4988-a540-fbde144d52ae",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent (SGDRegressor)\n",
    "\n",
    "SGDRegressor is another method incorporated into the model selection due to its computational efficiency and ability to scale well with large datasets, which is crucial for time series data that involve numerous observations like TD tracks. SGDRegressor is a linear model optimized via stochastic gradient descent and is particularly effective in scenarios where the data is large and sparse.\n",
    "\n",
    "For this project, **SGDRegressor** is beneficial for making fast predictions and learning iteratively over time-series data, especially when feature sets are large and complex. While not as adept at handling non-linear relationships out-of-the-box compared to Random Forest and Gradient Boosting, the linear nature of SGDRegressor can be leveraged when combined with:\n",
    "\n",
    "- Polynomial Features: To introduce non-linearity by adding interaction terms.\n",
    "- Feature Scaling: Ensuring that all features like latitude, longitude, wind speed, and pressure are normalized to ensure faster convergence during training.\n",
    "Moreover, SGDRegressor can benefit from incremental learning, meaning it can be updated with new data as it arrives (e.g., real-time tropical depression data), making it a good fit for a project where data evolves over time. However, SGDRegressor is more sensitive to hyperparameter tuning (e.g., learning rate, regularization strength), and it may require careful tuning to avoid issues like underfitting or overfitting. In this context, the model may require careful tuning of regularization parameters (e.g., L2 regularization), learning rate, and iteration count to ensure accurate predictions for TD tracks and intensities.\n",
    "\n",
    "**Workflow**\n",
    "1. Data Preparation\n",
    "- prep_data: This is a custom function, passed to the pipeline via the FunctionTransformer. It handles essential preprocessing, such as cleaning and extracting relevant features, ensuring the data is ready for model training.\n",
    "- prep_train_data: Another custom function transformer that prepares the training dataset, handling transformations such as shifting velocity and direction for consistency and ensuring target variables are correctly aligned for supervised learning.\n",
    "\n",
    "2. Preprocessing and Feature Engineering\n",
    "- One-Hot Encoding of ENSO Phases: The ENSO (El Niño–Southern Oscillation) phases significantly affect the behavior of tropical depressions. To capture this, we apply one-hot encoding to the enso column. This process transforms categorical data into a binary matrix, where each phase (El Niño, La Niña, Neutral) is represented as a separate binary feature. One-hot encoding ensures that the categorical nature of ENSO is well-represented in the model without introducing unintended ordinal relationships.\n",
    "\n",
    "- Polynomial Features: To capture non-linear relationships between features, PolynomialFeatures with a degree of 3 are applied to all columns. These polynomial features allow the model to detect complex, higher-order interactions between variables, which could be crucial for accurately predicting the trajectory and intensity of tropical depressions.\n",
    "\n",
    "3. Feature Scaling:\n",
    "After polynomial features are generated, we apply MinMaxScaler to all relevant features. This ensures that all variables are on the same scale, which is particularly important for machine learning algorithms like SGDRegressor, which are sensitive to feature scaling. The scaling operation ensures that no feature dominates the learning process due to its magnitude.\n",
    "\n",
    "4. Model Training: SGDRegressor with Grid Search\n",
    "\n",
    "We use SGDRegressor as the primary learning algorithm for the model, wrapped in a MultiOutputRegressor to handle multiple output features (latitude, longitude, wind speed, etc.). The SGDRegressor is trained with the following hyperparameters:\n",
    "- learning_rate: Adaptive, to adjust learning rate over time.\n",
    "- penalty: L1 regularization, to enforce sparsity and prevent overfitting.\n",
    "- alpha: Small value to control the strength of the regularization.\n",
    "- max_iter: Set to 3000 iterations to allow for sufficient training convergence.\n",
    "- tol: Tolerance for optimization stopping criteria.\n",
    "\n",
    "A Grid Search was conducted to fine-tune these hyperparameters, optimizing performance and avoiding overfitting.\n",
    "\n",
    "5. Data Splitting and Model Evaluation\n",
    "\n",
    "The dataset is split into training and testing sets using a custom split_dataframe function with GroupShuffleSplit (gss). This ensures that all observations for a given tropical depression (grouped by ID) are kept together during the split. This approach prevents data leakage between training and test sets.\n",
    "\n",
    "6. Training and Scoring:\n",
    "\n",
    "After preprocessing, the pipeline is fit on the training data (X_train, y_train). The model is then evaluated using the test set (X_test, y_test), achieving a score of 83.85%. This accuracy is considered good for predicting the position, wind speed, and pressure of tropical depressions over the next 6-hour interval.\n",
    "\n",
    "7. Visualization and Tracking of Predicted vs. Real Positions\n",
    "\n",
    "To evaluate the model visually, the function plot_tds is used to plot the predicted and real tracks of the tropical depressions. The accuracy of predicted positions over the next 6 hours appears to match well with the real observations, indicating good performance of the model in predicting future positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "873c064d-2488-44a0-92d8-6918743a5f19",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3ce98a7-6723-40f5-9ba8-02aa7ae59b96",
   "metadata": {},
   "source": [
    "prep_data = FunctionTransformer(func=prepare_dataframe)\n",
    "prep_train_data = FunctionTransformer(func=generate_training_dataframe)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec4d6bcb-c6cd-4b2b-affb-5b7e7a256925",
   "metadata": {},
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),  # One-hot encode the ENSO feature\n",
    "        ('poly', PolynomialFeatures(degree=3, include_bias=False), slice(0, None)),\n",
    "    ], remainder='passthrough', force_int_remainder_cols=False)\n",
    "scaler = MinMaxScaler()\n",
    "regressor = MultiOutputRegressor(SGDRegressor(learning_rate='adaptive', max_iter=3000, penalty='l1', alpha=1e-6, tol=1e-5, random_state=97))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04fb7ece-fe8f-4251-907b-24c5e6114e42",
   "metadata": {},
   "source": [
    "jma_pred_pipeline_sgd = Pipeline([\n",
    "        ('prep_data', prep_data),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "jma_train_pipeline_sgd = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor)\n",
    "    ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bdba7cbc-baef-4b46-9389-1bf6ddd0267c",
   "metadata": {},
   "source": [
    "df_trn = prep_data.transform(df)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0cdc43d4-5a1f-43ed-b1d8-2a782e02d9ea",
   "metadata": {},
   "source": [
    "df_trn = prep_train_data.transform(df_trn)\n",
    "X_train, X_test, y_train, y_test = next(split_dataframe(df_trn, splitter='gss'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "620c503b-b83b-45b8-b575-14b17b76b670",
   "metadata": {},
   "source": [
    "jma_train_pipeline_sgd.fit(X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "356ee3e3-1c17-4877-a60d-b2008eba1719",
   "metadata": {},
   "source": [
    "jma_train_pipeline_sgd.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7f55f600-3afc-4720-a326-ac99dc3ead0d",
   "metadata": {},
   "source": [
    "X_test.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "38bcad5f-3e85-4acd-ba56-3cb9258b55c6",
   "metadata": {},
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d84246c4-04ae-4a38-a5ed-2e10ab3224be",
   "metadata": {},
   "source": [
    "plot_tds(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1dfc108-0e3d-4705-bfd4-f9f8b80e63ef",
   "metadata": {},
   "source": [
    "y_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf49f8ae-d314-445a-96f0-72b82b8693ca",
   "metadata": {},
   "source": [
    "X_pred = jma[jma.index == '2023-10-14 18:00:00']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cffae58-eecd-4ab1-bca6-aad36f003c63",
   "metadata": {},
   "source": [
    "X_expected = jma[jma.index == '2023-10-15 00:00:00']"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6a65c7b-a3da-479e-a4fa-d1329b70def3",
   "metadata": {},
   "source": [
    "X_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "85ca3e34-da88-40ee-b36f-d2659bc532fe",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83f014b6-227d-401c-bfe2-6f3b8c156c11",
   "metadata": {},
   "source": [
    "jma_pred_pipeline_sgd.predict(X_pred)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e5e1764-edfa-4c54-8f9e-d370109deff4",
   "metadata": {},
   "source": [
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "acf9e7aa-49cf-4556-8900-f66569e3b694",
   "metadata": {},
   "source": [
    "df_ = create_real_pred_df(jma, X_test, model=jma_pred_pipeline_sgd, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "ad9aa8f0-8794-4316-9ca6-b2ccf69b052d",
   "metadata": {},
   "source": [
    "**Method to Calculate Probability of Predicted Position**\n",
    "\n",
    "To calculate the probability of the predicted position being within a certain range of the actual position, we can use a statistical approach based on error distributions. The error distribution is calculated between predicted and real positions for the test dataset. Then we fit a normal distribution to the error values in terms of latitude and longitude differences. The standard deviation (σ) of the error distribution is used to estimate the confidence interval.\n",
    "\n",
    "Using the fitted distribution, we calculate the probability that the actual position falls within a certain radius of the predicted position.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a81b1ccb-07ca-4c98-a3ba-6da56b07fa5c",
   "metadata": {},
   "source": [
    "df_= probability_within_radius(df_, 10)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b9a8a3f4-4187-4d40-ae96-1d7a95decdeb",
   "metadata": {},
   "source": [
    "df_.probability_within_radius.min(), df_.probability_within_radius.max()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "588ead7e-7550-4073-ae62-3b3550df65a3",
   "metadata": {},
   "source": [
    "df_[df_.group == 127]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "34b016f0-12f9-4280-8d9f-c01768ccea88",
   "metadata": {},
   "source": [
    "X_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f612de00-0cd5-4470-b6b0-88d7c8e372c8",
   "metadata": {},
   "source": [
    "X_test[X_test.velocity_kn > 15][30:50]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea4f5ed8-052c-422e-87ef-a19dadd1cfd6",
   "metadata": {},
   "source": [
    "X_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4cd3368f-2917-4097-b537-7c12d63793a6",
   "metadata": {},
   "source": [
    "X_pred = jma[jma.index == '1952-10-11 12:00:00']\n",
    "X_expected = jma[jma.index == '1952-10-11 18:00:00']\n",
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31e0f556-b18d-4522-85f1-6091006e92b7",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "023c3e9e-81c2-4943-bc9c-f3ac88177f20",
   "metadata": {},
   "source": [
    "### RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2f3d8f06-113a-4ad7-8762-8f7326549fcb",
   "metadata": {},
   "source": [
    "preprocessor_rf = ColumnTransformer([\n",
    "        ('enso', OneHotEncoder(), ['enso']),  # One-hot encode the ENSO feature\n",
    "        ('poly', PolynomialFeatures(degree=3, include_bias=False), slice(0, None)),\n",
    "    ], remainder='passthrough', force_int_remainder_cols=False)\n",
    "scaler = MinMaxScaler()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7bcf7136-2389-49f2-bf7c-f275618c144e",
   "metadata": {},
   "source": [
    "regressor_rf = MultiOutputRegressor(RandomForestRegressor(n_estimators=100, random_state=97))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "be598375-9e7e-463a-a98e-f2d811f78388",
   "metadata": {},
   "source": [
    "jma_pred_pipeline_rf = Pipeline([\n",
    "        ('prep_data', prep_data),\n",
    "        ('preprocessor', preprocessor_rf),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor_rf)\n",
    "    ])\n",
    "\n",
    "jma_train_pipeline_rf = Pipeline([\n",
    "        ('preprocessor', preprocessor_rf),\n",
    "        ('scaler', scaler),\n",
    "        ('regressor', regressor_rf)\n",
    "    ])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "44c7133b-70e7-43ba-ac2a-463d601cc609",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test = next(split_dataframe(df_trn, splitter='gss'))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7111d128-9536-4f8b-8d8f-41ad82f2f9fc",
   "metadata": {},
   "source": [
    "jma_train_pipeline_rf.fit(X_train, y_train)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "497781fd-4723-4ab1-96d8-969aab09c369",
   "metadata": {},
   "source": [
    "jma_train_pipeline_rf.score(X_test, y_test)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30e7d4f3-596c-46a9-a799-88c54cde1f16",
   "metadata": {},
   "source": [
    "jma = pd.read_csv('data/csv_ready/jma_training', index_col=0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "223e87b5-d683-4db1-8302-4f4d899a5981",
   "metadata": {},
   "source": [
    "X_test"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9222f97a-1f88-46a5-a28e-9741c6549061",
   "metadata": {},
   "source": [
    "plot_tds(jma, X_test, model=jma_pred_pipeline_rf, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d6612f00-b428-4e22-bc08-300b802942d7",
   "metadata": {},
   "source": [
    "X_pred"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cd3db646-c525-42da-82ad-adf9cd5950bc",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75557c14-d712-4ff2-b4bc-d3b58a8f7b67",
   "metadata": {},
   "source": [
    "manage_prediction(X_pred, jma_pred_pipeline_rf)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6bab319e-762d-4eb7-8836-10896473ee94",
   "metadata": {},
   "source": [
    "df_ = create_real_pred_df(jma, X_test, model=jma_pred_pipeline_rf, n_samples=3, random_state=97)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72cf933f-f29d-4bc8-a9c2-110da7c6fb13",
   "metadata": {},
   "source": [
    "df_pred = jma[jma.group == 1031]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eef6b930-5bc0-48e9-ac22-1fce3b81a780",
   "metadata": {},
   "source": [
    "df_pred = df_pred[df_pred.group == 1031].copy()\n",
    "\n",
    "X_pred = df_pred[1:2]\n",
    "X_expected = df_pred[2:3]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4da1a3e0-e618-4f3a-995a-7b8ce1b831ec",
   "metadata": {},
   "source": [
    "X_pred\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "23aa43af-31a7-4f5b-89b3-344a98b5038c",
   "metadata": {},
   "source": [
    "X_expected"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "cf0d9ba8-2b20-49a7-be2f-1806146c4d28",
   "metadata": {},
   "source": [
    "manage_prediction(X_pred, jma_pred_pipeline_sgd)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f269ae9-0390-4938-8d13-a8dcd3a61fed",
   "metadata": {},
   "source": [
    "df_trn[df_trn.group == 1031].columns"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5df01d61-9cab-4a8e-9126-6c9570b3a1a2",
   "metadata": {},
   "source": [
    "df_trn[df_trn.group == 1031]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacac44a-78b0-4638-91d9-239d39eb8116",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
